{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries required to test the neural network built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are testing the init_layers function. We are checking if the shape of the weight matrices is correct. If the test passes, we print \"All tests pass\".\n",
    "First we try a test case to see if the function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import init_layers\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(3, [3, 2, 2])\n",
    "    assert W[0].shape == (2, 3)\n",
    "    assert W[1].shape == (2, 2)\n",
    "    assert b[0].shape == (3, 1)\n",
    "    assert b[1].shape == (2, 1)\n",
    "    assert b[2].shape == (2, 1)\n",
    "\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it works as expected.\n",
    "Let's generalize the test case to check if the function is working correctly for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from main import init_layers\n",
    "\n",
    "L = random.randint(1, 10)\n",
    "dims = [random.randint(1, 10) for _ in range(L)]\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(L, dims)\n",
    "    for i in range(L - 1):\n",
    "        assert W[i].shape == (dims[i + 1], dims[i])\n",
    "        assert b[i].shape == (dims[i], 1)\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the feedforward function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import feed_forward, sigmoid\n",
    "\n",
    "def test_forward_propagation():\n",
    "\n",
    "    W = [np.array([[1.], [2.]]), np.array([[1., 2.]])]\n",
    "    b = [np.array([[0]]), np.array([[1.], [2.]]), np.array([[3.]])]\n",
    "\n",
    "    L=3\n",
    "    # n = [1, 2, 1] number of neurons\n",
    "    X = np.array([[1.]])\n",
    "    \n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "\n",
    "    # We calculated by hand the expected results, to check if we obtain the matching values\n",
    "\n",
    "    assert Z[1][0] == 2\n",
    "    assert A[1][0] == sigmoid(Z[1][0])\n",
    "\n",
    "    assert Z[1][1] == 4\n",
    "    assert A[1][1] == sigmoid(Z[1][1])\n",
    "    \n",
    "    assert Z[2] == W[1] @ A[1] + b[2]\n",
    "    assert sigmoid(Z[2]) == y_hat\n",
    "\n",
    "    \"\"\" print(\"------------------\")\n",
    "    print(W[1] @ A[1] + b[1])\n",
    "    print(Z[2])\n",
    "    print(\"------------------\") \"\"\"\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case does work as expected.\n",
    "Let's now generalize the test to check if the feed forward function works well for any layer and number of neurons per layer.\n",
    "\n",
    "Then, we'll generalize the test once again to test the robustness of the function based on the dimension of the input matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 57],\n",
      "       [152]]), array([[1.00000000e+00],\n",
      "       [1.00000000e+00],\n",
      "       [4.79185020e-44],\n",
      "       [9.92842643e-01]]), array([[0.70292219],\n",
      "       [0.50832704]]), array([[0.60843109],\n",
      "       [0.91661474],\n",
      "       [0.06489641],\n",
      "       [0.71618326]]), array([[0.40562819]]), array([[0.1906221],\n",
      "       [0.7773233]]), array([[0.74067612],\n",
      "       [0.71289218],\n",
      "       [0.63446157]]), array([[0.3341784 ],\n",
      "       [0.77499239]]), array([[0.71953112]]), array([[0.79731743],\n",
      "       [0.91829424],\n",
      "       [0.30313632],\n",
      "       [0.71517225],\n",
      "       [0.35794066]])]\n",
      "[[0.79731743]\n",
      " [0.91829424]\n",
      " [0.30313632]\n",
      " [0.71517225]\n",
      " [0.35794066]]\n",
      "------------------\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "from main import feed_forward\n",
    "\n",
    "def test_forward_propagation_n():\n",
    "\n",
    "    # Limited to 10 layers - 10 layers probably won't ever be need in our case and we don't have infinite calculation power\n",
    "    L = rd.randint(3, 10)\n",
    "\n",
    "    dims = []\n",
    "    for i in range(L):\n",
    "        dims.append(rd.randint(1, 5))\n",
    "\n",
    "\n",
    "    # dims = [rd.randint(1, 10) for _ in range(L)] learn generators\n",
    "\n",
    "    W, b = init_layers(L, dims)\n",
    "    assert len(W) + 1 == len(b)\n",
    "\n",
    "    # number of lines of input matrix = n\n",
    "    # number of columns = 1 for now - robustness test afterwards\n",
    "    n1 = dims[0]\n",
    "\n",
    "    # Need to learn the use of generators\n",
    "    X_list = []\n",
    "    for i in range(n1):\n",
    "        X_list.append([rd.randint(1, 200)])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "\n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        assert np.array_equal(Z[i], W[i-1] @ A[i-1] + b[i])     \n",
    "        assert np.array_equal(A[i], sigmoid(Z[i]))\n",
    "\n",
    "    i = rd.randint(1, L)\n",
    "    # The values below are the same. As expected.\n",
    "    # print(W[i-1])\n",
    "    # print(A[i-1])\n",
    "    # print(b[i])\n",
    "    # print(W[i-1] @ A[i-1] + b[i])\n",
    "    # print(\"------------\")\n",
    "    # print(Z[i])                     # After calculating by hand, we can see this value is true\n",
    "    # print(\"------------\")\n",
    "    # print(\"------------\")\n",
    "    print(A)\n",
    "    print(y_hat)\n",
    "    \n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(\"All tests pass\")\n",
    "\n",
    "test_forward_propagation_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the backpropagation function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers\n",
    "nb_layers = 3\n",
    "dims = [2, 2, 1]\n",
    "W, b = init_layers(nb_layers, dims)\n",
    "\n",
    "# Input and target\n",
    "X = np.array([[1, 2], [3, 4]]).T\n",
    "y = np.array([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid_derivative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_derivative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_cross_entropy_derivative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot the losses\u001b[39;00m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Build/main.py:140\u001b[0m, in \u001b[0;36mtrain_nn\u001b[0;34m(nb_layers, X, y, nb_neurons, learning_rate, epochs, g, gder, loss, loss_derivative)\u001b[0m\n\u001b[1;32m    137\u001b[0m W, b \u001b[38;5;241m=\u001b[39m init_layers(nb_layers, nb_neurons)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 140\u001b[0m     A, Z, y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     grad_W, grad_b \u001b[38;5;241m=\u001b[39m backpropagation(nb_layers, X, y, W, b, A, Z, y_hat, g, gder, loss_derivative)\n\u001b[1;32m    142\u001b[0m     W, b \u001b[38;5;241m=\u001b[39m update_parameters(W, b, grad_W, grad_b, learning_rate)\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Build/main.py:79\u001b[0m, in \u001b[0;36mfeed_forward\u001b[0;34m(nb_layers, X, W, b, g)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Neuron values updated with the forward pass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nb_layers):\n\u001b[0;32m---> 79\u001b[0m     Z[i] \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m b[i]\n\u001b[1;32m     80\u001b[0m     A[i] \u001b[38;5;241m=\u001b[39m g(Z[i])\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A, Z, A[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "# Overfitting test \n",
    "from main import train_nn\n",
    "from main import sigmoid, sigmoid_derivative\n",
    "from main import binary_cross_entropy, binary_cross_entropy_derivative\n",
    "\n",
    "X = np.array([[0, 1]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "# Train the model\n",
    "losses = main.train_nn(3, X, y, [2, 1, 1], 0.1, 1000, g=sigmoid, gder=sigmoid_derivative, loss=binary_cross_entropy, loss_derivative=binary_cross_entropy_derivative)\n",
    "# Plot the losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss function\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
