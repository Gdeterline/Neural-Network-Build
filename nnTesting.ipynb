{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries required to test the neural network built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are testing the init_layers function. We are checking if the shape of the weight matrices is correct. If the test passes, we print \"All tests pass\".\n",
    "First we try a test case to see if the function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import init_layers\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(3, [3, 2, 2])\n",
    "    assert W[0].shape == (2, 3)\n",
    "    assert W[1].shape == (2, 2)\n",
    "    assert b[0].shape == (3, 1)\n",
    "    assert b[1].shape == (2, 1)\n",
    "    assert b[2].shape == (2, 1)\n",
    "\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it works as expected.\n",
    "Let's generalize the test case to check if the function is working correctly for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from main import init_layers\n",
    "\n",
    "L = random.randint(1, 10)\n",
    "dims = [random.randint(1, 10) for _ in range(L)]\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(L, dims)\n",
    "    for i in range(L - 1):\n",
    "        assert W[i].shape == (dims[i + 1], dims[i])\n",
    "        assert b[i].shape == (dims[i], 1)\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the feedforward function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import feed_forward, sigmoid\n",
    "\n",
    "def test_forward_propagation():\n",
    "\n",
    "    W = [np.array([[1.], [2.]]), np.array([[1., 2.]])]\n",
    "    b = [np.array([[0]]), np.array([[1.], [2.]]), np.array([[3.]])]\n",
    "\n",
    "    L=3\n",
    "    # n = [1, 2, 1] number of neurons\n",
    "    X = np.array([[1.]])\n",
    "    \n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "\n",
    "    # We calculated by hand the expected results, to check if we obtain the matching values\n",
    "\n",
    "    assert Z[1][0] == 2\n",
    "    assert A[1][0] == sigmoid(Z[1][0])\n",
    "\n",
    "    assert Z[1][1] == 4\n",
    "    assert A[1][1] == sigmoid(Z[1][1])\n",
    "    \n",
    "    assert Z[2] == W[1] @ A[1] + b[2]\n",
    "    assert sigmoid(Z[2]) == y_hat\n",
    "\n",
    "    \"\"\" print(\"------------------\")\n",
    "    print(W[1] @ A[1] + b[1])\n",
    "    print(Z[2])\n",
    "    print(\"------------------\") \"\"\"\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case does work as expected.\n",
    "Let's now generalize the test to check if the feed forward function works well for any layer and number of neurons per layer.\n",
    "\n",
    "Then, we'll generalize the test once again to test the robustness of the function based on the dimension of the input matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 58],\n",
      "       [127],\n",
      "       [  9],\n",
      "       [ 69]]), array([[7.99109036e-152],\n",
      "       [1.18635714e-032],\n",
      "       [1.58523555e-068]]), array([[0.49765765],\n",
      "       [0.80063397],\n",
      "       [0.63623976],\n",
      "       [0.51462428]]), array([[0.80309738],\n",
      "       [0.67409184]]), array([[0.05365224],\n",
      "       [0.15059632],\n",
      "       [0.51574798]]), array([[0.53989888],\n",
      "       [0.20044873]]), array([[0.2004548 ],\n",
      "       [0.78519145]]), array([[0.40221944],\n",
      "       [0.58620409],\n",
      "       [0.47826865],\n",
      "       [0.68699578]]), array([[0.82828535],\n",
      "       [0.82162712],\n",
      "       [0.76819194],\n",
      "       [0.82686906],\n",
      "       [0.2191913 ]])]\n",
      "[[0.82828535]\n",
      " [0.82162712]\n",
      " [0.76819194]\n",
      " [0.82686906]\n",
      " [0.2191913 ]]\n",
      "------------------\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "from main import feed_forward\n",
    "\n",
    "def test_forward_propagation_n():\n",
    "\n",
    "    # Limited to 10 layers - 10 layers probably won't ever be need in our case and we don't have infinite calculation power\n",
    "    L = rd.randint(3, 10)\n",
    "\n",
    "    dims = []\n",
    "    for i in range(L):\n",
    "        dims.append(rd.randint(1, 5))\n",
    "\n",
    "\n",
    "    # dims = [rd.randint(1, 10) for _ in range(L)] learn generators\n",
    "\n",
    "    W, b = init_layers(L, dims)\n",
    "    assert len(W) + 1 == len(b)\n",
    "\n",
    "    # number of lines of input matrix = n\n",
    "    # number of columns = 1 for now - robustness test afterwards\n",
    "    n1 = dims[0]\n",
    "\n",
    "    # Need to learn the use of generators\n",
    "    X_list = []\n",
    "    for i in range(n1):\n",
    "        X_list.append([rd.randint(1, 200)])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "\n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        assert np.array_equal(Z[i], W[i-1] @ A[i-1] + b[i])     \n",
    "        assert np.array_equal(A[i], sigmoid(Z[i]))\n",
    "\n",
    "    i = rd.randint(1, L)\n",
    "    # The values below are the same. As expected.\n",
    "    # print(W[i-1])\n",
    "    # print(A[i-1])\n",
    "    # print(b[i])\n",
    "    # print(W[i-1] @ A[i-1] + b[i])\n",
    "    # print(\"------------\")\n",
    "    # print(Z[i])                     # After calculating by hand, we can see this value is true\n",
    "    # print(\"------------\")\n",
    "    # print(\"------------\")\n",
    "    print(A)\n",
    "    print(y_hat)\n",
    "    \n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(\"All tests pass\")\n",
    "\n",
    "test_forward_propagation_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the backpropagation function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers\n",
    "nb_layers = 3\n",
    "dims = [2, 2, 1]\n",
    "W, b = init_layers(nb_layers, dims)\n",
    "\n",
    "# Input and target\n",
    "X = np.array([[1, 2], [3, 4]]).T\n",
    "y = np.array([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[26.76682227, 35.51070296],\n",
      "       [16.83195986, 22.36952755]]), array([[-0.8930159 ,  0.01018276]])]\n",
      "[array([[0.],\n",
      "       [0.]]), array([[8.74388068],\n",
      "       [5.53756769]]), array([[-0.89970463]])]\n"
     ]
    }
   ],
   "source": [
    "def plot_gradient_histograms(grad_W, grad_b):\n",
    "    \"\"\"\n",
    "    Plots histograms of the gradients for weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    grad_W: List of weight gradients for each layer.\n",
    "    grad_b: List of bias gradients for each layer.\n",
    "    \"\"\"\n",
    "    num_layers = len(grad_W)\n",
    "\n",
    "    plt.figure(figsize=(12, num_layers * 4))\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        # Plot weight gradients\n",
    "        plt.subplot(num_layers, 2, i * 2 + 1)\n",
    "        plt.hist(grad_W[i].flatten(), bins=50, alpha=0.75, color='blue')\n",
    "        plt.title(f'Layer {i+1} Weight Gradients')\n",
    "        plt.xlabel('Gradient Value')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "        # Plot bias gradients\n",
    "        plt.subplot(num_layers, 2, i * 2 + 2)\n",
    "        plt.hist(grad_b[i].flatten(), bins=50, alpha=0.75, color='orange')\n",
    "        plt.title(f'Layer {i+1} Bias Gradients')\n",
    "        plt.xlabel('Gradient Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "    plt.savefig(\"./images/gradient_histograms.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after backpropagation\n",
    "A, Z, y_hat = main.feed_forward(nb_layers, X, W, b)\n",
    "grad_W, grad_b = main.backpropagation(nb_layers, X, y, W, b, A, Z, main.sigmoid_derivative, main.binary_cross_entropy_derivative)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(grad_W)\n",
    "print(grad_b)\n",
    "\n",
    "#plot_gradient_histograms(grad_W, grad_b)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
