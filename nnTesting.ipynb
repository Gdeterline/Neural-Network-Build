{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries required to test the neural network built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are testing the init_layers function. We are checking if the shape of the weight matrices is correct. If the test passes, we print \"All tests pass\".\n",
    "First we try a test case to see if the function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of features:  3\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import init_layers\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(3, [3, 2, 2])\n",
    "    assert W[0].shape == (2, 3)\n",
    "    assert W[1].shape == (2, 2)\n",
    "    assert b[0].shape == (3, 1)\n",
    "    assert b[1].shape == (2, 1)\n",
    "    assert b[2].shape == (2, 1)\n",
    "\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it works as expected.\n",
    "Let's generalize the test case to check if the function is working correctly for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of features:  2\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from main import init_layers\n",
    "\n",
    "L = random.randint(1, 10)\n",
    "dims = [random.randint(1, 10) for _ in range(L)]\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(L, dims)\n",
    "    for i in range(L - 1):\n",
    "        assert W[i].shape == (dims[i + 1], dims[i])\n",
    "        assert b[i].shape == (dims[i], 1)\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the feedforward function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion going on\n",
      "Assertation value ok\n",
      "(2, 1)\n",
      "(1, 1)\n",
      "[[1.]]\n",
      "(1, 2)\n",
      "(2, 1)\n",
      "[[0.88079708]\n",
      " [0.98201379]]\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import feed_forward, sigmoid\n",
    "\n",
    "def test_forward_propagation():\n",
    "\n",
    "    W = [np.array([[1.], [2.]]), np.array([[1., 2.]])]\n",
    "    b = [np.array([[0]]), np.array([[1.], [2.]]), np.array([[3.]])]\n",
    "\n",
    "    L=3\n",
    "    # n = [1, 2, 1] number of neurons\n",
    "    X = np.array([[1.]])\n",
    "    \n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "\n",
    "    # We calculated by hand the expected results, to check if we obtain the matching values\n",
    "\n",
    "    assert Z[1][0] == 2\n",
    "    assert A[1][0] == sigmoid(Z[1][0])\n",
    "\n",
    "    assert Z[1][1] == 4\n",
    "    assert A[1][1] == sigmoid(Z[1][1])\n",
    "    \n",
    "    assert Z[2] == W[1] @ A[1] + b[2]\n",
    "    assert sigmoid(Z[2]) == y_hat\n",
    "\n",
    "    \"\"\" print(\"------------------\")\n",
    "    print(W[1] @ A[1] + b[1])\n",
    "    print(Z[2])\n",
    "    print(\"------------------\") \"\"\"\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case does work as expected.\n",
    "Let's now generalize the test to check if the feed forward function works well for any layer and number of neurons per layer.\n",
    "\n",
    "Then, we'll generalize the test once again to test the robustness of the function based on the dimension of the input matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of features:  5\n",
      "X: [[134]\n",
      " [152]\n",
      " [ 39]\n",
      " [200]\n",
      " [  4]]\n",
      "Assertion going on\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected shape of X: number of samples in line, number of features in column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43mtest_forward_propagation_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m, in \u001b[0;36mtest_forward_propagation_n\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_list)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[1;32m---> 32\u001b[0m A, Z, y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Z) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(A)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A) \u001b[38;5;241m==\u001b[39m L\n",
      "File \u001b[1;32mc:\\Users\\g.macquartdeterline\\Documents\\GitHub\\Neural-Network-Build\\main.py:72\u001b[0m, in \u001b[0;36mfeed_forward\u001b[1;34m(nb_layers, X, W, b, g)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward\u001b[39m(nb_layers, X, W, b, g\u001b[38;5;241m=\u001b[39msigmoid):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssertion going on\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(W[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape of X: number of samples in line, number of features in column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssertation value ok\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Initialize the input layer\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected shape of X: number of samples in line, number of features in column"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "from main import feed_forward\n",
    "\n",
    "def test_forward_propagation_n():\n",
    "\n",
    "    # Limited to 10 layers - 10 layers probably won't ever be need in our case and we don't have infinite calculation power\n",
    "    L = rd.randint(3, 10)\n",
    "\n",
    "    dims = []\n",
    "    for i in range(L):\n",
    "        dims.append(rd.randint(1, 5))\n",
    "\n",
    "\n",
    "    # dims = [rd.randint(1, 10) for _ in range(L)] learn generators\n",
    "\n",
    "    W, b = init_layers(L, dims)\n",
    "    assert len(W) + 1 == len(b)\n",
    "\n",
    "    # number of lines of input matrix = n\n",
    "    # number of columns = 1 for now - robustness test afterwards\n",
    "    n1 = dims[0]\n",
    "\n",
    "    # Need to learn the use of generators\n",
    "    X_list = []\n",
    "    for i in range(n1):\n",
    "        X_list.append([rd.randint(1, 200)])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "\n",
    "    print(\"X:\", X)\n",
    "\n",
    "    A, Z, y_hat = feed_forward(L, X, W, b)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        assert np.array_equal(Z[i], W[i-1] @ A[i-1] + b[i])     \n",
    "        assert np.array_equal(A[i], sigmoid(Z[i]))\n",
    "\n",
    "    i = rd.randint(1, L)\n",
    "    # The values below are the same. As expected.\n",
    "    # print(W[i-1])\n",
    "    # print(A[i-1])\n",
    "    # print(b[i])\n",
    "    # print(W[i-1] @ A[i-1] + b[i])\n",
    "    # print(\"------------\")\n",
    "    # print(Z[i])                     # After calculating by hand, we can see this value is true\n",
    "    # print(\"------------\")\n",
    "    # print(\"------------\")\n",
    "    print(A)\n",
    "    print(y_hat)\n",
    "    \n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(\"All tests pass\")\n",
    "\n",
    "test_forward_propagation_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the backpropagation function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of features:  2\n",
      "X: [[1 2]]\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize layers\n",
    "nb_layers = 3\n",
    "dims = [2, 1, 1]\n",
    "W, b = main.init_layers(nb_layers, dims)\n",
    "\n",
    "# Input and target\n",
    "X = np.array([[1, 2]])\n",
    "y = np.array([[1, 0]])\n",
    "\n",
    "print(\"X:\", X)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in entry data:  2\n",
      "[[1 2]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 2)\n",
      "Expected number of features:  2\n",
      "Assertion going on\n",
      "Assertation value ok\n",
      "(2, 2)\n",
      "(2, 1)\n",
      "[[1]\n",
      " [2]]\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "[[0.95131207 0.95131207]\n",
      " [0.13439432 0.13439432]]\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "[[0.69413816 0.69413816]\n",
      " [0.45167304 0.45167304]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features in entry data: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m epochs, losses \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid_derivative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_derivative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_cross_entropy_derivative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(epochs)\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Build/main.py:158\u001b[0m, in \u001b[0;36mtrain_nn\u001b[0;34m(nb_layers, X, y, nb_neurons, learning_rate, epochs, g, gder, loss, loss_derivative)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    157\u001b[0m     A, Z, y_hat \u001b[38;5;241m=\u001b[39m feed_forward(nb_layers, X, W, b, g)\n\u001b[0;32m--> 158\u001b[0m     grad_W, grad_b \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_derivative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     W, b \u001b[38;5;241m=\u001b[39m update_parameters(W, b, grad_W, grad_b, learning_rate)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWff: \u001b[39m\u001b[38;5;124m\"\u001b[39m, W)\n",
      "File \u001b[0;32m~/Documents/GitHub/Neural-Network-Build/main.py:127\u001b[0m, in \u001b[0;36mbackpropagation\u001b[0;34m(nb_layers, X, y, W, b, A, Z, y_hat, g, gder, loss_derivative)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Backpropagate through layers\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 127\u001b[0m     grad_W[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m \u001b[38;5;241m/\u001b[39m nb_samples\n\u001b[1;32m    128\u001b[0m     grad_b[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m nb_samples\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "# Overfitting test \n",
    "from main import train_nn\n",
    "from main import sigmoid, sigmoid_derivative\n",
    "from main import binary_cross_entropy, binary_cross_entropy_derivative, mean_squared_error, mean_squared_error_derivative\n",
    "\n",
    "X = np.array([[1, 2]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "print(\"Number of features in entry data: \", len(X[0]))\n",
    "\n",
    "# Train the model\n",
    "epochs, losses = main.train_nn(4, X, y, [2, 2, 2, 1], 0.1, 10000, g=sigmoid, gder=sigmoid_derivative, loss=binary_cross_entropy, lossder=binary_cross_entropy_derivative)\n",
    "\n",
    "print(losses)\n",
    "print(epochs)\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(epochs, losses)\n",
    "plt.xlabel(\"Epoch i\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss function\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have an issue with epoch 1. Let's try to debug the issue.\n",
    "We will decompose the train function to proceed step by step (ie. epoch by epoch) and see where the issue is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in entry data:  2\n",
      "[[1 2]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 2)\n",
      "Expected number of features:  2\n",
      "Assertion going on\n",
      "Assertation value ok\n",
      "(1, 2)\n",
      "(2, 1)\n",
      "[[1]\n",
      " [2]]\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "[[0.62520358]]\n",
      "W:  [array([[-0.72841006,  0.42522004]]), array([[0.60581614]])]\n",
      "b:  [array([[0.],\n",
      "       [0.]]), array([[0.40493033]]), array([[-0.09285938]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from main import init_layers, feed_forward, backpropagation, update_parameters, sigmoid, sigmoid_derivative, binary_cross_entropy, binary_cross_entropy_derivative\n",
    "\n",
    "learning_rate = 0.1\n",
    "nb_layers = 3\n",
    "nb_neurons = [2, 1, 1]\n",
    "\n",
    "# epoch 0 - epochs is set to 1 \n",
    "\n",
    "X = np.array([[1, 2]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "print(\"Number of features in entry data: \", len(X[0]))\n",
    "\n",
    "nb_samples = len(X)\n",
    "\n",
    "print(X)\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "\n",
    "#### Improvement suggestions - automatise the number of neurons in the input layer based on the number of features of the input data\n",
    "W, b = init_layers(nb_layers, nb_neurons)\n",
    "\n",
    "\n",
    "A, Z, y_hat = feed_forward(nb_layers, X, W, b, g=sigmoid)\n",
    "grad_W, grad_b = backpropagation(nb_layers, X, y, W, b, A, Z, y_hat, g=sigmoid, gder=sigmoid_derivative, lossder=binary_cross_entropy_derivative)\n",
    "W, b = update_parameters(W, b, grad_W, grad_b, learning_rate)\n",
    "\n",
    "print(\"W: \", W)\n",
    "print(\"b: \", b)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified the problem - the issue is with the update of the bias matrices. The list of biaises is as follows:\n",
    "[(2, 1), (2, 1), (1, 1)]\n",
    "\n",
    "Since our neural network is of three layers composed with [2, 1, 1] neurons, we expect to have the following list of biases:\n",
    "[(2, 1), (1, 1), (1, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  [array([[0.],\n",
      "       [0.]]), array([[1.52077846]]), array([[-1.1297131]])]\n",
      "b:  [array([[0.],\n",
      "       [0.]]), array([[1.93021476]]), array([[-1.28675655]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"b: \", b)\n",
    "\n",
    "for i in range(len(grad_b)-1):\n",
    "    b[i+1] = b[i+1] - learning_rate * grad_b[i+1]\n",
    "    \n",
    "print(\"b: \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously b[i+1] = b[i+1] - learning_rate * grad_b[i] doesn't work because the dimensions of the matrices before and after calculation are different.\n",
    "\n",
    "This makes sense because the dimensions of the matrices are different between the layers. We need to update the bias matrices in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
