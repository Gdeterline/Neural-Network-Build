{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are testing the init_layers function. We are checking if the shape of the weight matrices is correct. If the test passes, we print \"All tests pass\".\n",
    "First we try a test case to see if the function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import init_layers\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(3, [3, 2, 2])\n",
    "    assert W[0].shape == (2, 3)\n",
    "    assert W[1].shape == (2, 2)\n",
    "    assert b[0].shape == (3, 1)\n",
    "    assert b[1].shape == (2, 1)\n",
    "    assert b[2].shape == (2, 1)\n",
    "\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it works as expected.\n",
    "Let's generalize the test case to check if the function is working correctly for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from main import init_layers\n",
    "\n",
    "L = random.randint(1, 10)\n",
    "dims = [random.randint(1, 10) for _ in range(L)]\n",
    "\n",
    "def test_init_layers():\n",
    "    W, b = init_layers(L, dims)\n",
    "    for i in range(L - 1):\n",
    "        assert W[i].shape == (dims[i + 1], dims[i])\n",
    "        assert b[i].shape == (dims[i], 1)\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_init_layers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is working correctly for all cases - random number of layers and random number of neurons in each layer.\n",
    "\n",
    "Now we can test the feedforward function. We will test the function with a simple test case to see if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]]), array([[2.],\n",
      "       [4.]]), array([[5.84482466]])]\n",
      "------------------\n",
      "[[5.84482466]]\n",
      "[[5.84482466]]\n",
      "------------------\n",
      "All tests pass\n"
     ]
    }
   ],
   "source": [
    "from main import feed_forward_test, sigmoid\n",
    "\n",
    "def test_forward_propagation():\n",
    "\n",
    "    W = [np.array([[1.], [2.]]), np.array([[1., 2.]])]\n",
    "    b = [np.array([[1.], [2.]]), np.array([[3.]])]\n",
    "\n",
    "    L=3\n",
    "    # n = [1, 2, 1] number of neurons\n",
    "    X = np.array([[1.]])\n",
    "    \n",
    "    A, Z, output = feed_forward_test(L, X, W, b)\n",
    "    print(Z)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "\n",
    "    # We calculated by hand the expected results, to check if we obtain the matching values\n",
    "\n",
    "    assert Z[1][0] == 2\n",
    "    assert A[1][0] == sigmoid(Z[1][0])\n",
    "\n",
    "    assert Z[1][1] == 4\n",
    "    assert A[1][1] == sigmoid(Z[1][1])\n",
    "    \n",
    "    # The two following values are different which isn't normal - need to check ([[5.84482466]] and 0)\n",
    "    assert Z[2] == W[1] @ A[1] + b[1]\n",
    "    print(\"------------------\")\n",
    "    print(W[1] @ A[1] + b[1])\n",
    "    print(Z[2])\n",
    "    print(\"------------------\")\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case does work as expected.\n",
    "Let's now generalize the test to check if the feed forward function works well for any layer and number of neurons per layer.\n",
    "\n",
    "Then, we'll generalize the test once again to test the robustness of the function based on the dimension of the input matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mtest_forward_propagation_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtest_forward_propagation_n\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     X_list\u001b[38;5;241m.\u001b[39mappend([rd\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m200\u001b[39m)])\n\u001b[0;32m     29\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_list)\n\u001b[1;32m---> 31\u001b[0m A, Z, output \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Z) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(A)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A) \u001b[38;5;241m==\u001b[39m L\n",
      "File \u001b[1;32mc:\\Users\\g.macquartdeterline\\Documents\\GitHub\\Neural-Network-Build\\main.py:87\u001b[0m, in \u001b[0;36mfeed_forward\u001b[1;34m(nb_layers, nb_neurons, X, g)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#print(W)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#print(\"----------------\")\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#print(b)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Neuron values updated with the forward pass\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(W)):\n\u001b[1;32m---> 87\u001b[0m     Z[i] \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m b[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     88\u001b[0m     A[i] \u001b[38;5;241m=\u001b[39m g(Z[i])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A, Z, A[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 8)"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "from main import feed_forward\n",
    "\n",
    "def test_forward_propagation_n():\n",
    "\n",
    "    # Limited to 10 layers - 10 layers probably won't ever be need in our case and we don't have infinite calculation power\n",
    "    L = rd.randint(1, 10)\n",
    "\n",
    "    dims = []\n",
    "    for i in range(L):\n",
    "        dims.append(rd.randint(1, 10))\n",
    "\n",
    "\n",
    "    # dims = [rd.randint(1, 10) for _ in range(L)] learn generators\n",
    "\n",
    "    W, b = init_layers(L, dims)\n",
    "    assert len(W) + 1 == len(b)\n",
    "\n",
    "\n",
    "    # number of lines of input matrix = n\n",
    "    # number of columns = 1 for now - robustness test afterwards\n",
    "    n1 = dims[0]\n",
    "\n",
    "    # Need to learn the use of generators\n",
    "    X_list = []\n",
    "    for i in range(n1):\n",
    "        X_list.append([rd.randint(1, 200)])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "\n",
    "    A, Z, output = feed_forward(L, dims, X, g=sigmoid)\n",
    "\n",
    "    assert len(Z) == len(A)\n",
    "    assert len(A) == L\n",
    "\n",
    "    \n",
    "    # We calculated by hand the expected results, to check if we obtain the matching values\n",
    "\n",
    "    #assert Z[1][0] == 2\n",
    "    #assert A[1][0] == sigmoid(Z[1][0])\n",
    "\n",
    "    #assert Z[1][1] == 4\n",
    "    #assert A[1][1] == sigmoid(Z[1][1])\n",
    "    \n",
    "    # The two following values are different which isn't normal - need to check ([[5.84482466]] and 0)\n",
    "    #assert Z[2] == W[1] @ A[1] + b[1]\n",
    "    print(\"------------------\")\n",
    "    #print(W[1] @ A[1] + b[1])\n",
    "    #print(Z[2])\n",
    "    #print(len(A))\n",
    "\n",
    "    print(\"------------------\")\n",
    "    print(\"All tests pass\")\n",
    "    \n",
    "test_forward_propagation_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[  9]\n",
      " [159]\n",
      " [137]\n",
      " [ 51]\n",
      " [ 86]\n",
      " [ 42]\n",
      " [  8]\n",
      " [ 24]]\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "# Limited to 10 layers - 10 layers probably won't ever be need in our case and we don't have infinite calculation power\n",
    "L = rd.randint(1, 10)\n",
    "\n",
    "dims = []\n",
    "for i in range(L):\n",
    "    dims.append(rd.randint(1, 10))\n",
    "\n",
    "n1 = dims[0]\n",
    "\n",
    "X_list = []\n",
    "for i in range(n1):\n",
    "    X_list.append([rd.randint(1, 200)])\n",
    "\n",
    "X = np.array(X_list)\n",
    "\n",
    "print(type(X))\n",
    "print(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
